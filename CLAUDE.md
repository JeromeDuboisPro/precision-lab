# Precision Lab - Project Context for Claude Code

## ğŸ¯ Project Mission

**Explore precision-performance tradeoffs in numerical computing through interactive visualization**

This project demonstrates how reduced floating-point precision (FP8/FP16/FP32/FP64) affects convergence of numerical algorithms - a critical question for modern GPU math libraries and AI accelerators.

---

## ğŸ”§ Project Management with Beads

This project uses **beads** (`bd` CLI) for lightweight issue tracking with dependency support.

### Quick Reference
```bash
bd list          # List all issues
bd show <id>     # Show issue details
bd ready         # Show unblocked work
bd blocked       # Show blocked issues
bd close <id>    # Close completed issue
```

### âš ï¸ CONFIDENTIAL PROTOCOL
**CRITICAL**: A git pre-commit hook automatically guards against accidental exposure.

The hook blocks commits containing sensitive terms. If triggered:
1. Review flagged files
2. Remove or rephrase sensitive content
3. Re-stage and commit

**Public framing**: Educational research project exploring precision-performance frontiers.

---

## ğŸ“ Mathematical Foundation

### Power Method Algorithm
Iterative algorithm for computing dominant eigenvalue Î»â‚ of matrix A:

1. Start with random vector xâ‚€
2. Iterate: x_{k+1} = AÂ·x_k / ||AÂ·x_k||
3. Eigenvalue estimate: Î» = x_k^TÂ·AÂ·x_k
4. Converge when residual norm < tolerance

**Convergence Rate**: The power method converges linearly with rate Ï = |Î»â‚‚/Î»â‚|

- **Convergence ratio**: Ï = |Î»â‚‚/Î»â‚| (second to first eigenvalue)
- **Error reduction per iteration**: error_k â‰ˆ Ïáµ Â· error_0
- **Iterations to precision Îµ**: k â‰ˆ log(Îµ) / log(Ï)
- **Example**: Ï = 0.9 â†’ ~44 iterations per decade of accuracy

The `convergence_type` parameter controls Ï:
- `"fast"` â†’ Ï â‰ˆ 0.5 (Î»â‚‚/Î»â‚ gap = 50%)
- `"slow"` â†’ Ï â‰ˆ 0.909 (Î»â‚‚/Î»â‚ gap = 10%)

### âš ï¸ IMPORTANT: Use Residual Norm, NOT Relative Error

**Always use normalized residual ||Av - Î»v|| / (|Î»| Â· ||v||) as the convergence metric.**

- **Normalized Residual**: ||Av - Î»v|| / (|Î»| Â· ||v||) measures convergence independent of scale
- Uses |Î»| as approximation for ||A||â‚‚ (valid for SPD matrices where ||A||â‚‚ = Î»_max)
- It's the mathematically proper convergence criterion for iterative eigensolvers
- Shows correct precision floor behavior for each floating-point format

### Condition Number Îº
Ratio of largest to smallest eigenvalue: Îº = Î»_max / Î»_min

- **Well-conditioned** (Îº < 100): Fast, stable convergence
- **Moderately conditioned** (100 â‰¤ Îº â‰¤ 1000): Slower convergence
- **Ill-conditioned** (Îº > 1000): Very slow, sensitive to precision

---

## ğŸ”¬ Precision Formats

### FP64 (Double Precision)
- **Format**: 1 sign + 11 exponent + 52 mantissa bits
- **Machine Epsilon**: 2.22e-16
- **Use**: Scientific computing, reference baseline

### FP32 (Single Precision)
- **Format**: 1 sign + 8 exponent + 23 mantissa bits
- **Machine Epsilon**: 1.19e-7
- **Use**: Most engineering/ML training

### FP16 (Half Precision)
- **Format**: 1 sign + 5 exponent + 10 mantissa bits
- **Machine Epsilon**: 9.77e-4
- **Use**: ML training, well-conditioned problems

### FP8 (via ml_dtypes)
- **E4M3 Format**: 1 sign + 4 exponent + 3 mantissa bits
- **E5M2 Format**: 1 sign + 5 exponent + 2 mantissa bits
- **Machine Epsilon**: E4M3: ~0.125, E5M2: ~0.25
- **Use**: ML training/inference on modern GPU tensor cores

---

## ğŸ—ï¸ Project Structure

```
precision-lab/
â”œâ”€â”€ src/precision_lab/          # Python package
â”‚   â”œâ”€â”€ algorithms/             # Numerical algorithms
â”‚   â”‚   â””â”€â”€ power_method/       # Power method implementations
â”‚   â”œâ”€â”€ precision/              # FP8/16/32/64 handling
â”‚   â””â”€â”€ visualization/          # Trace generation
â”œâ”€â”€ docs/                       # GitHub Pages
â”‚   â”œâ”€â”€ index.html             # Landing page
â”‚   â”œâ”€â”€ race.html              # Precision race visualization
â”‚   â””â”€â”€ cascading.html         # Cascading precision demo
â”œâ”€â”€ experiments/                # Reproducible scripts
â”œâ”€â”€ tests/                      # pytest suite
â””â”€â”€ .github/workflows/          # CI + Pages deployment
```

---

## âš¡ Key Algorithms

### Standard Power Method
Compare convergence across FP8/FP16/FP32/FP64 for same matrix.

### Cascading Precision (Novel Contribution)
Dynamic precision escalation: **FP8 â†’ FP16 â†’ FP32 â†’ FP64**

**Strategy**:
1. Start at FP8 (fastest throughput)
2. Detect stagnation or precision threshold
3. Escalate to next precision level
4. Carry eigenvector state across transitions

**Results**: 2-3Ã— speedup vs FP64-only for same accuracy.

---

## ğŸ¯ H100 Performance Modeling

### Time Speedup (Simulated)
**Note**: These are *theoretical maximum* speedup factors for demonstration purposes.
Actual performance varies based on memory bandwidth, matrix size, and implementation.

Scale CPU time to simulate GPU performance:
- **FP8**: 6Ã— speedup (theoretical tensor core peak)
- **FP16**: 4Ã— speedup (theoretical half-precision units)
- **FP32**: 1Ã— (baseline)
- **FP64**: 1Ã— (reference)

*Real-world power method is memory-bound, not compute-bound. Actual speedups
may be lower depending on memory bandwidth utilization.*

### Iteration Budget (Fair Comparison)
Allocate more iterations to faster precisions:
- **FP8**: 6Ã— iterations
- **FP16**: 4Ã— iterations
- **FP32**: 2Ã— iterations
- **FP64**: 1Ã— (baseline)

---

## âœ… Quality Standards

### Numerical Correctness
- Convergence must satisfy precision-appropriate tolerance
- Residual norm computed correctly
- State preserved across precision transitions

### Performance Claims
- Fair comparison (same matrix, same seed)
- Correct FLOPS count (2nÂ² + n per iteration)
- **Never run benchmarks in parallel** (corrupts timing)

### Code Quality
- Type hints throughout
- Google-style docstrings
- pytest test coverage
- ruff + mypy clean

---

## ğŸ“š Key References

### Numerical Methods
- Golub & Van Loan: "Matrix Computations" (power method theory)
- Higham: "Accuracy and Stability of Numerical Algorithms"

### Mixed Precision
- Micikevicius et al.: "Mixed Precision Training" (ICLR 2018)
- IEEE 754 Standard: Floating-Point Arithmetic

---

## ğŸš¦ Development Workflow

### Session Start
```bash
bd list               # ALWAYS check open beads first!
bd ready              # See actionable tasks
bd show <bead-id>     # Check details
```

> **âš ï¸ IMPORTANT**: Always run `bd list` at session start to check open beads before starting work.

### During Work
- Validate mathematical correctness
- Run tests frequently
- Document uncertainties

### Before Commit
```bash
# Pre-commit hook runs automatically - no manual check needed
pytest tests/
ruff check .
mypy src/
```

### Session End
```bash
bd close <completed-beads>
git add -A && git commit
```

---

## ğŸ“Š Visualization Data Generation

### Trace Generation Parameters
When regenerating traces for the interactive visualizations:

```python
# Configuration for fair comparison
matrix_size = 1024           # 1024Ã—1024 matrix
condition_number = 100.0     # Îº=100 (moderately conditioned)
seed = 42                    # Reproducibility
convergence_type = "slow"    # 10% eigenvalue gap (Î»â‚‚/Î»â‚ = 0.909)
```

### Convergence Targets
- **Cascading**: `target_residual=1e-12` â†’ Forces use of all 4 precision levels (FP8â†’FP16â†’FP32â†’FP64)
- **FP64 reference**: `target_error=1e-12` â†’ Must match cascading's residual target for fair comparison

### Expected Results (1024Ã—1024, Îº=100)
| Method | Raw Iterations | Effective Iterations | Final Residual |
|--------|---------------|---------------------|----------------|
| Cascading | ~283 | ~165 | ~9.45e-13 |
| FP64-only | ~259 | 259 | ~9.81e-13 |

**Speedup**: Cascading achieves same accuracy in ~165 effective iterations vs FP64's 259 = **1.57Ã— faster**

### Effective Iteration Calculation
X-axis shows "Effective FP64 Iterations" (normalized by speedup):
- FP8 iterations Ã· 6
- FP16 iterations Ã· 4
- FP32 iterations Ã· 2
- FP64 iterations Ã· 1 (baseline)

---

**This is an educational research project demonstrating precision-performance tradeoffs in numerical computing.**
